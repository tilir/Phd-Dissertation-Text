\chapter{Обзор подходов к компиляции гетерогенных приложений}\label{ch:overview}

\section{Логическое и физическое представление гетерогенных систем}\label{sec:overview/logical}

\subsection{Логическая модель памяти для GPGPU}\label{subsec:overview/logical/memory}

При работе с графическими API для вычислений (такими как OpenCL, SYCL, Cuda и прочие), программисту приходится иметь дело с несколькими уровнями памяти.
Базовое разделение это \textbf{память хоста} -- память хостовой машины, обычно это что-то вроде обычной RAM и \textbf{память устройства}, устроенная гораздо сложнее.

В обычном случае память устройства бывает следующих видов:

\begin{itemize}
\item \textbf{Глобальная} -- память на устройстве обычно большого объёма и доступная каждому EU.
\item \textbf{Виртуальная разделяемая} -- участки памяти непосредственно отображаемые на память хоста.
\item \textbf{Константная} -- память, изменение которой в программах выполняемых на устройстве запрещено.
\item \textbf{Локальная} -- память доступная только потокам внутри рабочей группы.
\item \textbf{Приватная} -- память (обычно очень небольшое её количество) доступная только конкретному EU.
\end{itemize}

Здесь бывают тонкости и нюансы в терминологии. Например память, которая в SYCL называется приватной, в CUDA называется локальной, локальная называется разделяемой, а разделяемая глобальная называется унифицированной. Но с точностью до переименований все современные программные интерфейсы работают более-менее с одной логической моделью. Далее в этой работе будет использоваться терминология SYCL.

Обычно тип памяти напрямую запрашивается при работе с программным интерфейсом как на листинге~\cref{lst:oclapi}. Такие вызовы с хоста по сути являются обращениями к графическому рантайму или как его иначе называют UMD (user-mode driver). Он обрабатывает такого рода запросы и комбинируя их в пакеты отправляет их к нижележащему интерфейсу операционной системы.

\nomenclature{\(UMD\)}{user-mode driver, обычное название графического рантайма}
\nomenclature{\(KMD\)}{kernel-mode driver, драйвер операционной системы, требующий привилегированного исполнения}

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Пример запроса глобального буффера в OpenCL API}\label{lst:oclapi}
    \begin{lstlisting}[language={[ISO]C++}]
  cl_context Context;
  cl_mem Buf;
  cl_int Err;
  Context = clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU, NULL, 
                                    NULL, &Err);
  // .....
  Buf = clCreateBuffer(Context, CL_MEM_READ_WRITE, BUFSZ * sizeof(int), 
                       NULL, &Err);
    \end{lstlisting}
\end{ListingEnv}

Мы будем называть такого рода вызовы API-вызовами (API call). Многие современные программные интерфейсы такие как SYCL и CUDA делают эти вызовы неявными. Другие, такие как OpenCL, наоборот, стараются делать явные вызовы. В хорошо спроектированной программе количество такого рода вызовов надо минимизировать так как такого рода общение с рантаймом не бесплатно и при неправильном использовании вносит ненужные расходы времени и памяти.

В данном случае API-вызов \lstinline!clCreateBuffer! запрашивает создание буфера в глобальной памяти. В некоторых случаях компилятор может оптимизировать работу с памятью, например сделав stateless to stateful преобразование и тогда этот буфер окажется в несколько другой памяти (при соблюдении ряда условий). Но программист всегда оперирует с логической моделью, так как это единственный способ написать общий код для нескольких устройств гибридной системы.

\begin{figure}[ht]
    \centerfloat{
        \includegraphics[scale=0.6]{Vladimirov/images/logical-memory.pdf}
    }
    \caption{Логическая модель памяти}\label{fig:logical-memory}
\end{figure}

Логическая модель памяти, характерная для таких API, как OpenCL и SYCL, представлена на рисунке~\cref{fig:logical-memory}. Ключевым понятием является computing unit (CU), комбинирующий несколько исполнительных устройств. Каждый computing unit это элемент кооперативной многозадачности внутри GPU. Все исполнительные устройства в пределах CU обладают доступом к общему пространству -- локальной памяти. Это небольшая и быстрая общая память. Она очень важна в таком виде. Но ещё важнее, что итерационное пространство рабочей группы позволяет потокам в её пределах ждать друг друга на семафорах, называемых часто барьерами.

Чтобы проиллюстрировать концепцию барьера, допустим у нас есть гнездо циклов с внутренним параллелизмом, представленное на листинге~\cref{lst:nobarrier}.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Гнездо циклов с внутренним параллелизмом}\label{lst:nobarrier}
    \begin{lstlisting}[language={[ISO]C++}]
for (auto i : Ni)
  for (auto j : Nj)
    parallel for (auto k : Nk)
      do(i, j, k);
    \end{lstlisting}
\end{ListingEnv}

Это довольно плохой случай, так как мы можем распараллелить только самый вложенный цикл и получается, что мы должны постоянно входить в параллельное исполнение и выходить из него. Но если порядок по $k$ не важен, и $N_k$ не велико, то мы можем воспользоваться барьерами рабочей группы. Тогда это гнездо циклов трансформируется в следующий более выгодный вид, приведённый на листинге~\cref{lst:localbarrier}.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Гнездо циклов с барьером внутри}\label{lst:localbarrier}
    \begin{lstlisting}[language={[ISO]C++}]
parallel for (auto k : Nk)
  for (auto i : Ni)
    for (auto j : Nj) {
       do(i, j, k);
       barrier(k);
    }
    \end{lstlisting}
\end{ListingEnv}

Разница в том что теперь для каждого элемента параллелизма оба дорогих вложенных цикла исполняются на своём исполнительном устройстве. Эта техника на некоторых бенчамрках, таких как перемножение матриц или битоническая сортировка позволяют выиграть до тридцати процентов производительности и более даже в простом скалярном случае.

Чтобы сделать пример более реалистичным, рассмотрим типичное гнездо циклов для практически важной задачи перемножения матриц, приведенное на листинге~\cref{lst:gemmnested}. Матрицы разбиты на тайлы и для каждого тайла мы в первом цикле запускаем копирование в локальную память и вычисление небольшого произведения в локальной памяти (мы должны помнить что она не велика). Во втором цикле мы собираем результаты обратно в глобальную память.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Матричное перемножение с локальной памятью и вложенным параллелизмом}\label{lst:gemmnested}
    \begin{lstlisting}[language={[ISO]C++}]
private S = 0;
for (Tile = 0; Tile < NumTiles; ++Tile) {
  parallel for(auto Wi: Workitems)
    copy_global_to_local(Tile, Wi); 
  parallel for(auto Wi: Workitems)
    S = calculate_local_mmult(wi);
}
parallel for(auto Wi: Workitems)
  update_global_memory(S, Wi); 
    \end{lstlisting}
\end{ListingEnv}

Очевидно здесь внешний цикл никак не распараллелен и это приводит к существенным накладным расходам. Чтобы этого избежать, давайте формально введём барьеры и инвертируем циклы точно по той же схеме, которую мы использовали на листинге~\cref{lst:localbarrier}.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Матричное перемножение с барьерами}\label{lst:gemmbarrier}
    \begin{lstlisting}[language={[ISO]C++}]
parallel for(auto Wi: Workitems)
  for (Tile = 0; Tile < NumTiles; ++Tile) {
    copy_global_to_local(Tile, Wi);
    barrier;
  }
parallel for(auto Wi: Workitems)
  for (Tile = 0; Tile < NumTiles; ++Tile) {
    S = calculate_local_mmult(wi);
    barrier;
  }
    \end{lstlisting}
\end{ListingEnv}

Результат такого рода инверсии приведён на листинге~\cref{lst:gemmbarrier}. Теперь мы имеем два паралелльных внешних цикла и на каждое исполнительное устройство попадает довольно большая вычислительная задача: целый внутренний цикл. Внутри него стоит барьер на котором потоки ждут друг друга в пределах локальнйо рабочей группы -- то есть собственно в пределах работы над тайлом. Внимательно глядя на оба этих цикла мы замечаем, что теперь мы внезапно можем их скомбинировать и не ждать с обновлением глобальной памяти до конца работы.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Объединение параллельных циклов}\label{lst:gemmfinal}
    \begin{lstlisting}[language={[ISO]C++}]
parallel for(auto Wi: Workitems) {
  private S = 0;
  for (Tile = 0; Tile < NumTiles; ++Tile) {
    copy_global_to_local(Tile, Wi);
    barrier;
    S = calculate_local_mmult(wi);
    barrier;
  }
  update_global_memory(S, Wi);
}
    \end{lstlisting}
\end{ListingEnv}

Результат комбинации приведён на листинге~\cref{lst:gemmfinal}. Это довольно важная трансформация, так как мы убрали ещё одну ветку fork-join: теперь вместо того чтобы напрягать рантайм оффлоадом двух циклов, мы оффлоадим один, внутри которого ждём нужных событий на барьерах.

\subsection{Физическая модель памяти}\label{subsec:overview/logical/physmem}

Для того чтобы эффективно оптимизировать программы, работающие с памятью в пределах логической модели, компиляторы должны приниматьв  рассчёт физическое устройство памяти. Схема видеокарточки на рисунке~\cref{fig:typicalGPU} может быть обобщена, чтобы лучше показать связь между различными блоками и устройствами.

На физическом уровне память устройства делится на обладающую и не обладающую состоянием. Обладающая состоянием (stateful) память это такая память где каждый буфер имеет точку привязки (binding point) и есть поддержка в железе для быстрого преобразования binding table index (BTI) и смещения в буфере в физический адрес. У каждой адресации массива есть незримое состояние: для каждого индекса BTI есть реальный физический адрес куда отображена память и размер буфера по этому адресу, а иногда и нечто другое. Работа со stateful памятью за счёт аппаратной поддержки может быть очень эффективна, но фактически сырые (то есть stateless) указатели в такой модели оказываются запрещены: они позволяют слишком много.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Типичная программа на GLSL для Vulkan}\label{lst:typicalvulkan}
    \begin{lstlisting}[language={[ISO]C++}]
struct Particle { vec4 pos, vel; };
layout(std140, binding = 0) buffer Pos {
  Particle particles[];
};
layout (binding = 1) uniform UBO {
  float deltaT;
  int count;
} ubo;

// ....

for (int i = 0; i < ubo.count; i += DataSz)
  if (i + x < ubo.count)
    someData[x] = particles[i + x].pos;
    \end{lstlisting}
\end{ListingEnv}

На листинге~\cref{lst:typicalvulkan} приведена типичная программа на GLSL, фрагмент взят из симуляции системы частиц~\cite{gunadi2018real}. Мы видим структуру для частицы, две точки привязки $binding_0$ и $binding_1$, к которым привязаны униформный буфер (UBO) и разделяемый буфер (SSBO). Показан также цикл обработки, когда координаты частиц аккумулируются в разделяемый буфер (видимо речь о вершинном шейдере). Глядя на этот код программист на C++ мог бы подумать что речь в целом просто об операциях с массивами и указателями.

\nomenclature{\(UBO\)}{unified buffer object, буфер в графических API для представления небольших данных}
\nomenclature{\(SSBO\)}{shared buffer object, большой буфер в грфаических API, часто отображённый на глобальную память хоста}

\begin{ListingEnv}[!h]
  \captiondelim{ } 
  \caption{Пример недопустимых операций на GLSL}\label{lst:fakepointers}
  \begin{lstlisting}[language={[ISO]C++}]
int *uboPtr = &ubo.deltaT;
int *ssboPtr = &particles[i + x].pos;
int *somePtr = CTUnknownCondition() ? ssboPtr : uboPtr;
*somePtr = 8;
  \end{lstlisting}
\end{ListingEnv}

Листинг~\cref{lst:fakepointers} разоблачает эту иллюзию. Практически все показанные на нём операции не являются легальными! Мы не можем взять указатель на объект внутри буфера так как если бы мы могли, то мы могли бы и смешать эти указатели в операции над указателями, то есть стереть информацию о точке привязки. И в этом случае у графического рантайма настали бы тяжёлые времена: ему бы потребовалось разрешать чем же всё-таки является \lstinline!somePtr!.

Локальная память фактически является обладающей состоянием, так как в локальной памяти у нас есть её условное начало и размер, но нет сквозной адресации с глобальной памятью устройства и нет возможности отображать её на память хоста. Интересно, что stateful память это классика именно графических программных интерфейсов -- и OpenGL \cite{kessenich2016opengl} и Vulkan \cite{sellers2016vulkan} работают исключительно с ней, так как это эффективно с точки зрения аппаратного ускорения и хорошо ложится на задачи рендеринга и обработки изображений. Разумеется когда появляется необходимость отображать память с хостовой машины, где никаких точек привязки уже нет, а, напротив, есть такие вещи как указатели и даже динамические структуры, состоящие из указателей, требуется более сложная память, напоминающая память хоста. Именно тут проходит важный водораздел между графикой и вычислениями на GPU.

\begin{figure}[ht]
  \centerfloat{
    \includegraphics[scale=0.4]{Vladimirov/images/memory-scheme.pdf}
  }
  \caption{Схема физической памяти}\label{fig:memory-scheme}
\end{figure}

На рисунке~\ref{fig:memory-scheme} изображена реалистичная схема физической памяти. Локальная память показана отдельно, так как по сути это кусок L1 кеша на каждом computing unit. Собственно о локальной памяти и можно думать как о своего рода кеше.

Интересно, что само наличие памяти обладающей состоянием является для вычислительных программных интерфейсов в основном прозрачным. Компилятор часто может сделать stateless to stateful преобразование, если он может доказать, что работа ведётся в пределах одного буфера. С другой стороны известно, что сложные модели памяти часто делают компиляторные оптимизации затруднительными или невозможными \cite{vafeiadis2015common}.

Интересной концепцией является приватная память. То о чём программист на высокоуровневом графическом или вычислительном API думает как о единой сущности: небольшом и быстром пространстве для сохранения промежуточных переменных внутри шейдера, как это видно из рисунка~\ref{fig:memory-scheme}, распадается на две части: действительно небольшой и быстрый регистровый файл и кусок собственно приватной памяти, выделяемой как часть глобальной памяти (и поэтому не особо быстродействующей даже в сравнении с локальной).

\subsection{Векторный характер графических систем команд}\label{subsec:overview/logical/hw}

Все гетерогенные вычисления нужны в основном затем, чтобы максимизировать производительность.
Для этих целей в средней видеокарточке используемой в гетерогенных системах обычно просто нет операционной системы -- её драйвер это драйвер операционной системы хоста.
В связи с эти для таких устройств (это верно и для GPU и для NPU) характерно.

\begin{enumerate}
\item Отсутствие накладных расходов на переключение контекста (нет OS).
\item Сравнительно далёкая и дорогая глобальная память.
\item Часто отсутствие кешей и короткий конвейер.
\end{enumerate}

Всё это мотивирует большие регистровые файлы.

\begin{figure}[ht]
    \centerfloat{
        \includegraphics[scale=0.6]{Vladimirov/images/genisa-addressing-base.pdf}
    }
    \caption{Схема регистрового файла}\label{fig:genisa-addressing-base}
\end{figure}

Схема регистрового файла представлена на рисунке~\cref{fig:genisa-addressing-base}

Вовсе не всегда работа с большими регистровыми файлами удобна.

\subsection{Векторный поток управления}\label{subsec:overview/logical/simdcf}

В графических ускорителях многих производителей есть возможность использовать векторный поток управления без сведения его к скалярному случаю. В архитектуре Intel Xe это реализовано через две команды: \texttt{goto} и \texttt{join}. Особенностью команды \texttt{goto} является возможность ее предикатирования. В этом случае исполнение программы произойдет так, что для предикатированных элементов вектора произойдет переход по указанной метке, а для всех остальных элементов произойдет переход на следующую инструкцию. Команда \texttt{join} восстанавливает поток управления.

В аппаратуре подобная модель исполнения реализуется с помощью маски исполнения (\textit{англ. execution mask}). Каждый бит этой маски соответствует элементу вектора. Если бит установлен в 0, то для соответствующего элемента вычисления не производятся, если же бит установлен в 1, то для соответствующего элемента исполнение происходит в обычном режиме. В самом начале программы маска состоит из единиц. Изменения в маске производят инструкции \texttt{goto} и \texttt{join}. Команда \texttt{goto} устанавливает в соответствии с предикатом биты маски в 0, выключая линии вектора, для которых условие не выполняется. Команда \texttt{join} восстанавливает маску до того состояния, в котором она находилась до изменения соответствующим \texttt{goto}.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Пример векторного потока управления}\label{lst:simdcf}
    \begin{Verb}
  ...
  (W)     cmp  (8|M0)  (gt)f0.0  null<1>:d  r2.0<8;8,1>:d -1:w
  (~f0.0) goto (8|M0)            BB_3
  BB_1:
  (W)     add  (8|M0)            r3.0<1>:d  r2.0<8;8,1>:d  1:w
  (f0.0)  goto (8|M0)            BB_5
  BB_2:
          join (8|M0)            BB_3
  BB_3:
          add  (8|M0)            r3.0<1>:d  r2.0<8;8,1>:d -1:w
  BB_4:
          join (8|M0)            BB_5
  BB_5:
  ...
    \end{Verb}
\end{ListingEnv}

На листинге~\cref{lst:oclapi}. пример кода c векторным потоком управления на языке ассемблера для архитектуры графических ускорителей Intel. В данном примере реализовано проверка того, что в регистре r.2.0 лежит значение большее, чем -1, а результат сравнения записан во флаговый регистр f0.0. Далее следует предикатированный регистром f0.0 условный переход: если условие для данной линии исполнения выполняется, то не происходит переход в базовый блок BB\_3, и к данным элементам прибавляется 1, а результат записывается в регистр r3.0. Для других элементов следует переход в базовый блок BB\_3, где из остальных элементов вычитается 1, а результат так же записывается в регистр r3.0.

\begin{figure}[ht]
    \centerfloat{
        \includegraphics[scale=0.5]{Vladimirov/images/HW-goto-example.pdf}
    }
    \caption{Схема векторного потока управления}\label{fig:HW-goto-example}
\end{figure}

На рисунке~\ref{fig:HW-goto-example} схематично изображено, как будет происходить исполнение данного кода. Цифрами сверху обозначены номера линий исполнения в SIMD АЛУ, зеленым обозначены включенные линии, после исполнения предшествовавшей инструкции, а оранжевым -- выключенные.

\section{Высокоуровневые векторные интерфейсы}\label{sec:overview/vectorapi}

\subsection{ISPC}\label{subsec:overview/vectorapi/ispc}

Язык программирования и система компиляции ISPC (Intel SPMD Program Compiler) построения для компиляции высокопроизводительного кода для гибридных систем. Изначально система была ориентирован в основном на CPU, но её концепции не менее эффективно могут быть представлены для современных GPU. Система построена так, чтобы масштабироваться как с ростом колчиества доступных ядер так и с увеличением доступной ширины векторов, легко интегрироваться в различные вычислительные окружения.

ISPC возник и до сих пор активно применяется для написания систем программной трассировки лучей \cite{moreau2019dynamic}. Изначально он был ориентирован на CPU \cite{pharr2013ray} в частности на расширения AVX512, но язык и сама модель SPMD программирования оказались настолько удачными \cite{pharr2018guest}, что его распространение на гетерогенные векторные системы стало вопросом времени.

Как и многие системы такого рода, IPSC имеет синтаксис похожий на синтаксис языка C, но с введёнными туда новыми концепциями uniform и varying переменных. Код выглядит скалярным, но каждая varying переменная (то есть любая не объявленная как uniform) является на самом деле вектором и может принимать разные значения в разных линиях маски выполнения. Базовой varying переменной является переменная \lstinline!programIndex!.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Пример программы на ISPC вычисляющей фрактал}\label{lst:ispcmandel}
    \begin{lstlisting}[language={[ISO]C++}]
void mandelbrot_ispc(uniform float x0, uniform float y0,
                     uniform float x1, uniform float y1,
                     uniform int width, uniform int height,
                     uniform int maxIterations,
                     uniform int output[]) {
  uniform float dx = (x1 - x0) / width, dy = (y1 - y0) / height;
  for (uniform int j = 0; j < height; j++) {
    for (uniform int i = 0; i < width; i += programCount) {
      float x = x0 + (programIndex + i) * dx;
      float y = y0 + j * dy;
      int index = j * width + i + programIndex;
      output[index] = mandel(x, y, maxIterations);
    }
  }
}
    \end{lstlisting}
\end{ListingEnv}

Пример программы на ISPC приведён на листинге~\cref{lst:ispcmandel}. Обратите внимание, что значения переменных \lstinline!x!, \lstinline!y!, \lstinline!index! на самом деле являются векторными. Это обеспечивает таким программам отличную производительность, позволяет смешивать скалярные и векторные вычисления, иметь единое когерентное пространство имён и встроенную в язык поддержку AoS и SoA.

\subsection{SYCL SIMD extensions}\label{subsec:overview/vectorapi/dpcppesimd}

ESIMD расширения в SYCL это расширения, которые позволяют для SYCL программирование с явным векторным представлением. Существовавшие до этого в SYCL подгруппы слишком сложны в обращении. Как пример, на листинге~\cref{lst:syclhisto} приведена часть реализации вычисления гистограммы на SYCL, использующая явные подгруппы.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Вычисление гистограммы на SYCL с явными подгруппами}\label{lst:syclhisto}
    \begin{lstlisting}[language={[ISO]C++}]
for (int I = N; I < NumData / NumBins; I += NGSZ) {
  for (int J = 0; J < NumBins; J += 1) {
    const int Base = G * LSZ + SGI * SLR;
    const int Idx = I * NumBins + Base + J * SGSize;
    const auto VData = SubGroup.load(BufferData + Idx);
    for (int K = 0; K < SGSize; K++) {
      auto Y = sycl::group_broadcast(SubGroup, VData, K);
      if (SLI == (Y % SGSize))
        PrivateHist[Y / SGSize] += 1;
    \end{lstlisting}
\end{ListingEnv}

Видно что мы имитируем векторное чтение с помощью \lstinline!SubGroup.load! и формируем значение, делая векторный регистр с помощью \lstinline!sycl::group_broadcast!. Это не слишком эффективно из-за лишних броадкастов и очень сложно в написании. Гораздо проще и эффективней работать сразу с векторами.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Работа с Explicit SIMD расширением SYCL}\label{lst:syclesimd}
    \begin{lstlisting}[language={[ISO]C++}]
constexpr unsigned VL = 16;
auto Kern = [=](sycl::id<1> Id) SYCL_ESIMD_KERNEL {
  unsigned Offset = Id * VL * sizeof(float);
  esimd::simd<T, VL> VA = esimd::block_load<T, VL>(A, Offset);
  esimd::simd<T, VL> VB = esimd::block_load<T, VL>(B, Offset);
  esimd::simd<T, VL> VC = VA + VB;
  esimd::block_store(C, Offset, VC);
};
    \end{lstlisting}
\end{ListingEnv}

На листинге~\cref{lst:syclesimd} приведён пример векторного сложения в DPC++ ESIMD. Здесь и далее под DPC++ будем понимать SYCL с некоторыми специальными расширениями \cite{reinders2021dpc}. В той же работе приведена достаточная мотивация появления таких расширений и замеры положительного эффекта на производительность. Также благодаря использованию явных векторных API для реализации приложений машинного обучения, видеокарта PVC \cite{gomes2022ponte} существенно превзошла более мощные по аппаратуре видеокарты от конкурирующих компаний на задачах inference для resnet50.

\section{Обзор существующих подходов к векторизации}\label{sec:overview/vectorizing}

Теперь когда мы познакомились с основными векторными программными интерфейсами, какзалось бы нет ничего проще чем отобразить их один в один на векторную же аппараутру. Чтобы разоблачить это заблуждение давайте посмотрим на существующие механизмы векторизации для скалярных и векторных программных интерфейсов.

Большая часть существующих процессоров показывают пиковую производительность благодаря той или иной форме векторизованных инструкций \cite{bialas2015benchmarking}. Это верно и для CPU, например Intel XEON с расширениями AVX и для GPU. Например AMD Graphics Cores Next имеет 64-линейные вектора, а NVIDIA CUDA имеет 32-линейные варпы. Варп \cite{passerat2015warp} это специальный термин для неявного параллелизма обеспечиваемого средствами аппаратного обеспечения. По самой природе таких инструкций, все операции над вектором выполняются одновремено (по крайней мере концептуально). Это ограничивает класс алгоритмов, который может быть эффективно реализован на таких архитектурах. Разумеется цена будет разной при разных подходах.

\subsection{Скалярная ISA и скалярное API}\label{subsec:overview/vectorizing/cuda}

Модель CUDA при реализации высокоуровневой программы делает крайне лёгким написание программы в которой векторный поток управления расходится. Это кстати будет далее характерно и для OpenCL и вообще это верно для любого скалярного API, полагающегося на неявную векторизацию. Она моделирует вычислительную задачу как большое количество потоков исполняющих одну небольшую программу, называемую кернелом или шейдером и для программиста это выглядит как многопоточное параллельное исполнение (SIMT).

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Пример кернела на CUDA с замером цикла}\label{lst:cudashader}
    \begin{lstlisting}[language={[ISO]C++}]
__global__ void single_loop(int* limits, float* out) {
  int tid = blockDim.x * blockIdx.x + threadIdx.x;
  int M = limits[threadIdx.x];
  for (int k = 0; k < N_UNROLL; ++k)
    for (int i = 0; i < M; i++)
      sum += EXPR_INNER;
  __syncthreads();
  out[tid] = sum;
}
    \end{lstlisting}
\end{ListingEnv}

Фактически многие руководство CUDA Programming Guide \cite{guide2013cuda} утверждает, что для корректности программист может в целом игнорировать SIMT поведение. Однако серьёзные улучшения производительности могут быть получены если учесть, требует ли код различные потоки внутри варпа расходиться.

На архитектуре NVIDIA потоки группируются в варпы по 32 потока каждый, которые должны исполнять одни и те же инструкции по сути в векторном виде. Из этого следует, что любая инструкция, которая выполняет условный переход, не дающая одного и того же значения в пределах варпа ведёт к неявному расхождению потока управления. В статье про архитектуру Tesla \cite{lindholm2008nvidia} упомянуты механизмы, которыми аппаратура пытается справляться с этой задачей, например стек синхронизации переходов.

\subsection{Векторная ISA и скалярные интерфейсы}\label{subsec:overview/vectorizing/sycl}

Начиная с разработки OpenCL как фактического стандарта гетерогенных вычислений \cite{munshi2011opencl}, можно отсчитывать время подъёма скалярных интерфейсов. Несмотря на ограниченную поддержку в таких интерфейсах векторных типов, там нет ни возможности завести вектора произвольной длины ни поддержку сложных высокоуровневых опрераций с векторами. Скалярными строго говоря были и ранние графические интерфейсы, такие как OpenGL.

Но в отличие от графических, вычислительные интерфейсы сразу ориентировались на прозрачную (или плоскую) модель памяти, единую на хосте и устройствах. Это и другие соображения (например корректность и безопасность типов по границам вычислительного интерфейса) привели к довольно быстрому подъёму скалярных интерфейсов с совмещенным исходным кодом, например SYCL \cite{reinders2021sycl}. 

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Пример кода на SYCL}\label{lst:syclshader}
    \begin{lstlisting}[language={[ISO]C++}]
auto Evt = Q.submit([&](sycl::handler &Cgh) {
  auto A = BufA.template get_access<sycl_read>(Cgh);
  auto B = BufB.template get_access<sycl_read>(Cgh);
  auto C = BufC.template get_access<sycl_write>(Cgh);

  auto Kernmul = [=](sycl::id<2> WorkItem) {
    const int Row = WorkItem.get(0);
    const int Col = WorkItem.get(1);
    T Sum = 0;
    for (int K = 0; K < AY; K++)
      Sum += A[Row][K] * B[K][Col];
    C[Row][Col] = Sum;
  };

  Cgh.parallel_for<class mmult<T>>(Csz, Kernmul);
});
    \end{lstlisting}
\end{ListingEnv}

На листинге~\cref{lst:syclshader} приведен пример довольно наивного перемножения матриц на SYCL. Основная проблема компиляции такого рода шейдеров заключается в том, что мы всё-таки компилируем их в векторную систему команд. То есть векторизация должна быть включена как шаг оптимизационного конвейера. Скалярный компилятор \cite{weiss1987study} даёт базовые возможности такого рода, но обычно включает векторизацию после основных оптимизаций как часть кодогенерации. Это логично вытекает из простого соображения: скалярный компилятор не способен оптимизировать векторный код, значит логично сначала оптимизировать, потом векторизовать.

Это создаёт различные проблемы. Наиболее известны проблемы с векторизацией внешних циклов \cite{vasudevan1982inner}. Ещё более существенной проблемой является влияние на распределение регистров. По разному автовекторизованный код вызывает разное регистровое давление. Из-за этого векторизация вынужденно включается в цикл попыток векторизации и распределения регистров, также люди изобретают различные эвристики и прочее. то есть с одного и того же кода мы можем получить как итоговый ассемблер векторизованный на максимальную ширину (например на $16$) так и на ширину поменьше но более выгодную с точки зрения распределителя (например на $8$). Это называется SIMDX-dispatch и хуже всего в нём то, что он находится вне контроля программиста. 

Таким образом упражнения на программирование в скалярном интерфейсе с векторной системой команд это почти всегда упражнение на подчинение своего рода волшебства. Программист никогда не знает приведут ли небольшеи изменения его кода к небольшим изменениям производительности или к огромным.

Большим шагом вперёд в развитии языка SYCL стал момент, когда им заинтересовалась компания Intel в рамках развития инцииативы OneAPI. Количество и качество предложенных в SYCL расширений оказалось таковым, что фактически сформировало новый язык DPC++ \cite{ashbaugh2020data}, впрочем в последнее время от этого названия уходят по мере того как новые и новые специфичные расширения становятся частью языка SYCL. Интерес к OneAPI также подогревается тем фактом, что она с одной стороны не зависит от вендора, а с другой стороны предоставляет достаточно возможностей для портирования шейдерного кода с проприетарных систем, например с CUDA \cite{christgau2020porting}.

\subsection{Векторное API с ручным управлением регистровым файлом}\label{subsec:overview/vectorizing/cm}

Для решения проблемы воспроизводимой производительности, обозначенной в \cref{subsec:overview/vectorizing/sycl} был разработан язык C for Media (он же C for Metal или просто CM). CM (C-for-Metal) \cite{lueh2021c} -- язык программирования, являющийся расширением С++ и явно реализующий концепцию SIMD. CM является языком с раздельным исходным кодом, то есть компиляция кода хоста и кода устройства происходит раздельно. На данный момент целевыми являются только архитектуры графических ускорителей Intel, но спецификация CM не является проприетарной.

\begin{ListingEnv}[!h]
    \captiondelim{ } 
    \caption{Пример кода на CM}\label{lst:cmshader}
    \begin{lstlisting}[language={[ISO]C++}]
template <typename T>
CM_NODEBUG _GENX_ inline void
transpose_matrix(matrix_ref<T, 8, 8> m1, matrix_ref<T, 8, 8> m2) {
    // 32 bit mask to control how to merge two vectors.
    uint32_t mask = 0x55555555;
    matrix_ref<T, 4, 16> t1 = m1.format<T, 4, 16>();
    matrix_ref<T, 4, 16> t2 = m2.format<T, 4, 16>();

    // j = 1
    t2.row(0).merge(t1.replicate<8, 1, 2, 0>(0, 0), 
                    t1.replicate<8, 1, 2, 0>(2, 0), mask);
    t2.row(1).merge(t1.replicate<8, 1, 2, 0>(0, 8), 
                    t1.replicate<8, 1, 2, 0>(2, 8), mask);
    t2.row(2).merge(t1.replicate<8, 1, 2, 0>(1, 0), 
                    t1.replicate<8, 1, 2, 0>(3, 0), mask);
    t2.row(3).merge(t1.replicate<8, 1, 2, 0>(1, 8), 
                    t1.replicate<8, 1, 2, 0>(3, 8), mask);

    // j = 2
    t1.row(0).merge(t2.replicate<8, 1, 2, 0>(0, 0), 
                    t2.replicate<8, 1, 2, 0>(2, 0), mask);
    t1.row(1).merge(t2.replicate<8, 1, 2, 0>(0, 8), 
                    t2.replicate<8, 1, 2, 0>(2, 8), mask);
    t1.row(2).merge(t2.replicate<8, 1, 2, 0>(1, 0),
                    t2.replicate<8, 1, 2, 0>(3, 0), mask);
    t1.row(3).merge(t2.replicate<8, 1, 2, 0>(1, 8),
                    t2.replicate<8, 1, 2, 0>(3, 8), mask);
    \end{lstlisting}
\end{ListingEnv}

Пример кода на CM приведён на листинге~\cref{lst:cmshader}. Это фрагмент более сложного кода для явного векторного транспонирования матрицы. Код написан с рассчётом на то, чтобы в точности умещаться в регистровый файл (и собственно производить транспонирование на векторных регистрах и совокупностью операций, поддержанных для векторных регистров).

Видно что даже для простого транспонирования используется крайне сложное векторное программирование. Для такого языка также характерна минимальная роль компилятора, например.

Главной особенностью CM являются новые встроенные типы: \lstinline!vector! (вектор), \lstinline!vector_ref! (ссылка на вектор), \lstinline!matrix! (матрица) и \lstinline!matrix_ref! (ссылка на матрицу). Каждый из этих типов является шаблонным. Типы \lstinline!vector! и \lstinline!vector_ref! имеют два шаблонных параметра: тип элемента и количество элементов. Типы \lstinline!matrix! и \lstinline!matrix_ref! имеют три шаблонных параметра: тип элемента, количество столбцов и количество строк матрицы. Типом элемента может быть любой примитивный целочисленный тип или тип числа с плавающей точкой.

\section{Постановка задачи оптимизирующей компиляции}\label{sec:overview/howtobetter}

В заключение поставим высокоуровневую задачу оптимизирующей компиляции из высокоуровневого векторного программного интерфейса.

\textbf{Первая часть задачи:} имея на входе высокоуровневое промежуточное представление, характеризующееся наличием векторов произвольной длины, различных адресных пространств и векторного потока управления, необходимо на выходе получить удобное для векторной аппаратуры представление на фиксированных регионах, при этом разрешив проблемы представления приватной памяти в условиях ограниченности ресурсов вычислительного устройства. Все обобщённые операции с памятью должны быть представлены в операциях устройства. Также векторные глобальные переменные должны быть разрешены в чтения и записи векторных регионов. Эту часть задачи можно назвать задачей функциональной корректности. Если она будет решена то программа может быть представлена, но не факт, что будет оптимальной или даже просто лучше той же семантически программы написанной для скалярного программного интерфейса.

Отсюда следует \textbf{вторая часть задачи:} в методологии первой части найти места для оптимизации промежуточного представления таким образом, чтобы получившаяся программа была представима оптимальным образом с точки использования ресурсов, особенно приватной памяти и регистрового файла. Оптимальным образом должны быть также представлены конструкции потока управления с учётом расходящихся векторных линий при выполнении условных операторов над частью вектора.

Рассмотрим недостатки рассмотренных выше существующих походов для решения этой задачи.

\section{Выводы}\label{sec:overview/outcome}

В первой главе дается общий обзор подходов к компиляции гетерогенных приложений. Рассматривается логическая и физическая модель памяти типичных графических ускорителей. Обосновывается векторный характер регистровых файлов и операций в графических ускорителях. Рассматриваются современные векторные программные интерфейсы, такие как IPSC и DPC++ESIMD, которые становятся актуальны в связи с увеличением сложности задач решаемых современными графическими ускорителями и появления необходимости явно векторизовать вычисления для того, чтобы эффективно класть их на любое устройство, поддерживающее векторизацию (в том числе современные гибридные устройства). 

Проведён подробный анализ существующих подходов к векторизации, который показывает недостаточность существующих методов для эффективного представления векторных программных интерфейсов. Рассмотрены случаи скалряной системы команд с поздней векторизацией или векторной системы команд, с поздней и ранней векторизацией. Видно, что либо векторизация происходит слишком поздно, что не даёт возможности эффективно оптимизировать код на высоком уровне, либо слишком рано, что выключает существующие скалярные оптимизацити.

Недостатки существующих подходов заключаются в их непригодности для отображения высокоуровневых векторных программных интерфейсов на векторную систему команд.

\FloatBarrier